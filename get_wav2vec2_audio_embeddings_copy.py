# -*- coding: utf-8 -*-
"""get_wav2vec2_audio_embeddings copy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/144pmc46m32ZsBvUAiQqUIzc9aZEiaDWa

Ноутбук для получения эмбеддингов текста с помощью Wav2Vec2 из HuggingFace (https://huggingface.co/facebook/wav2vec2-base-960h)

Я так понимаю, нет cls-токена.

Длина последовательностей не выровнена.
"""

import os

import pandas as pd
import torch
import torchaudio as ta
from tqdm import tqdm
from transformers import AutoModel, AutoProcessor, AutoModelForSpeechSeq2Seq

split = "train"

audio_dir = f"D:\\CREMA-D\\AudioWAV"
store_dir = f"D:\\CREMA-D\\wav2vec_embs"

model_path = 'facebook/wav2vec2-large-960h-lv60-self'

# processor = AutoProcessor.from_pretrained(model_path)
# model = AutoModel.from_pretrained(model_path).to("cuda")

# # Смотри в этой ячейке

# not_processed = []

# tgt_sampling_rate = 16000

# for fname in tqdm(os.listdir(audio_dir)):
#     path = os.path.join(store_dir, fname.replace(".wav", ".pt"))
#     if os.path.exists(path):
#         continue
#     audiopath = os.path.join(audio_dir, fname) #путь
#     try:
#         waveform, sampling_rate = ta.load(audiopath) 
#     except:
#         not_processed.append(audiopath)
#         continue
#     waveform = ta.functional.resample(waveform, sampling_rate, tgt_sampling_rate).squeeze(0)
#     input_values, attention_mask = processor(
#         waveform, 
#         sampling_rate=tgt_sampling_rate,
#         return_tensors="pt",
#         padding=True,
#         feature_size=1, 
#         do_normalize=True, 
#         return_attention_mask=True,
#     ).values()


#     with torch.no_grad():
#         output = model(
#             input_values=input_values.to("cuda"),
#             attention_mask=attention_mask.to("cuda"),
#         )
#     torch.save(output.last_hidden_state.to("cpu"), path)

# print(not_processed)

# Смотри в этой ячейке
# !pip install transformers
# from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq
tgt_sampling_rate = 16000

processor = AutoProcessor.from_pretrained("sanchit-gandhi/whisper-small-ru-1k-steps")
model = AutoModelForSpeechSeq2Seq.from_pretrained("sanchit-gandhi/whisper-small-ru-1k-steps")

# # !pip install datasets
# from datasets import load_dataset

# ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

# ds[0]["audio"]["array"].shape

if True:
    audiopath = "/content/russian_record.wav"
    waveform, sampling_rate = ta.load(audiopath) 
    waveform = waveform[0]#.unsqueeze(0)
    waveform = ta.functional.resample(waveform, sampling_rate, tgt_sampling_rate)
    input_values = processor(
        waveform, 
        sampling_rate=tgt_sampling_rate,
        return_tensors="pt",
        # padding=True,
        # feature_size=1, 
        # do_normalize=True, 
        # return_attention_mask=True,
     )

input_features = input_values.input_features

generated_ids = model.generate(inputs=input_features)

transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

transcription

"""# Новый раздел"""

from google.colab import drive
drive.mount('/content/drive')